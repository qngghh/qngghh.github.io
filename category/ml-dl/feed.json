{
    "version": "https://jsonfeed.org/version/1",
    "title": "Supernova • All posts by \"ml-dl\" category",
    "description": "小派过气画家",
    "home_page_url": "https://www.kitx86.com",
    "items": [
        {
            "id": "https://www.kitx86.com/2022/05/25/ml-dl-nlp-jing-sai-shu-ju-chu-li/",
            "url": "https://www.kitx86.com/2022/05/25/ml-dl-nlp-jing-sai-shu-ju-chu-li/",
            "title": "ML-DL-nlp竞赛数据处理",
            "date_published": "2022-05-25T14:11:03.000Z",
            "content_html": "<h3 id=\"kaggle-天池竞赛环境\"><a href=\"#kaggle-天池竞赛环境\" class=\"headerlink\" title=\"kaggle/天池竞赛环境\"></a>kaggle/天池竞赛环境</h3><p>1.设置一个训练、推理随机种子</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">seed_everything</span><span class=\"token punctuation\">(</span>seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n    random<span class=\"token punctuation\">.</span>seed<span class=\"token punctuation\">(</span>seed<span class=\"token punctuation\">)</span>\n    os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'PYTHONHASHSEED'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">(</span>seed<span class=\"token punctuation\">)</span>\n    os<span class=\"token punctuation\">.</span>environ<span class=\"token punctuation\">[</span><span class=\"token string\">'TF_CUDNN_DETERMINISTIC'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'1'</span>\n    np<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>seed<span class=\"token punctuation\">(</span>seed<span class=\"token punctuation\">)</span>\n    tf<span class=\"token punctuation\">.</span>random<span class=\"token punctuation\">.</span>set_seed<span class=\"token punctuation\">(</span>seed<span class=\"token punctuation\">)</span>\n    \nseed_everything<span class=\"token punctuation\">(</span>seed<span class=\"token operator\">=</span><span class=\"token number\">42</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>2.在nlp中大多数都是使用 deberta系列来完成训练的，很傻瓜只需要会用 model 来训练就好了</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\"># tokenizer 在推理时是非常重要的</span>\ntokenizer <span class=\"token operator\">=</span> transformers<span class=\"token punctuation\">.</span>AutoTokenizer<span class=\"token punctuation\">.</span>from_pretrained<span class=\"token punctuation\">(</span>config<span class=\"token punctuation\">.</span>model_name<span class=\"token punctuation\">)</span>\n\n<span class=\"token comment\"># Context tokens. </span>\ntest<span class=\"token punctuation\">[</span><span class=\"token string\">'context_token'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'['</span> <span class=\"token operator\">+</span> test<span class=\"token punctuation\">.</span>context <span class=\"token operator\">+</span> <span class=\"token string\">']'</span>\ntest<span class=\"token punctuation\">[</span><span class=\"token string\">'sep_token'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'[SEP]'</span>\ntest<span class=\"token punctuation\">[</span><span class=\"token string\">'cls_token'</span><span class=\"token punctuation\">]</span> <span class=\"token operator\">=</span> <span class=\"token string\">'[CLS]'</span>\ncontext_tokens <span class=\"token operator\">=</span> <span class=\"token builtin\">list</span><span class=\"token punctuation\">(</span>test<span class=\"token punctuation\">.</span>context_token<span class=\"token punctuation\">.</span>unique<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span>\ntokenizer<span class=\"token punctuation\">.</span>add_special_tokens<span class=\"token punctuation\">(</span><span class=\"token punctuation\">{</span><span class=\"token string\">'additional_special_tokens'</span><span class=\"token punctuation\">:</span> context_tokens<span class=\"token punctuation\">}</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n",
            "tags": [
                "kaggle",
                "nlp"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/17/ml-dl-shu-ju-jiang-wei-zhong-de-pca-he-lda/",
            "url": "https://www.kitx86.com/2022/05/17/ml-dl-shu-ju-jiang-wei-zhong-de-pca-he-lda/",
            "title": "ML-DL-数据降维中的PCA和LDA",
            "date_published": "2022-05-17T06:07:22.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/17/ml-dl-tong-ji-zhi-san-zhi-song-shu/",
            "url": "https://www.kitx86.com/2022/05/17/ml-dl-tong-ji-zhi-san-zhi-song-shu/",
            "title": "ML-DL-统计之三只松鼠",
            "date_published": "2022-05-17T06:05:35.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/14/ml-dl-zai-rnn-zhong-tian-jia-lstm-ji-zhi-lai-huan-he-ti-du-xiao-shi/",
            "url": "https://www.kitx86.com/2022/05/14/ml-dl-zai-rnn-zhong-tian-jia-lstm-ji-zhi-lai-huan-he-ti-du-xiao-shi/",
            "title": "ML-DL-在RNN中添加LSTM机制来缓和梯度消失",
            "date_published": "2022-05-14T14:22:59.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-bu-shou-xi-de-cai-yang-fang-fa/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-bu-shou-xi-de-cai-yang-fang-fa/",
            "title": "ML-DL-不熟悉的采样方法",
            "date_published": "2022-05-13T01:54:50.000Z",
            "content_html": "<h3 id=\"我来华丽的简单分类一下\"><a href=\"#我来华丽的简单分类一下\" class=\"headerlink\" title=\"我来华丽的简单分类一下\"></a>我来华丽的简单分类一下</h3><p>高斯分布采样</p>\n<p>马尔可夫蒙特卡洛采样</p>\n<p>贝叶斯网络采样</p>\n<p>不均衡样本集采样</p>\n",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-suan-fa-san-xiong-gui-zhi-sun-shi-han-shu/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-suan-fa-san-xiong-gui-zhi-sun-shi-han-shu/",
            "title": "ML-DL-算法三兄贵之损失函数",
            "date_published": "2022-05-13T01:53:45.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-you-hua-suan-fa-de-bu-fen-jia-she/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-you-hua-suan-fa-de-bu-fen-jia-she/",
            "title": "ML-DL-优化算法的部分假设",
            "date_published": "2022-05-13T01:52:44.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-rnn-zhong-you-gai-ru-he-hua-li-de-mo-shang-dropout/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-rnn-zhong-you-gai-ru-he-hua-li-de-mo-shang-dropout/",
            "title": "ML-DL-RNN中又该如何华丽的抹上Dropout",
            "date_published": "2022-05-13T01:39:57.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-sui-rnn-quan-chong-geng-xin-shi-chan-sheng-de-chang-qi-yi-lai-wen-ti/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-sui-rnn-quan-chong-geng-xin-shi-chan-sheng-de-chang-qi-yi-lai-wen-ti/",
            "title": "ML-DL-随RNN权重更新时产生的长期依赖问题",
            "date_published": "2022-05-13T01:37:59.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-rnn-de-jie-gou-ji-ru-he-you-mei-de-geng-xin-quan-chong/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-rnn-de-jie-gou-ji-ru-he-you-mei-de-geng-xin-quan-chong/",
            "title": "ML-DL-RNN的结构及如何优美的更新权重",
            "date_published": "2022-05-13T01:37:00.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-dropout-zen-yang-huan-jie-rnn-guo-ni-he/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-dropout-zen-yang-huan-jie-rnn-guo-ni-he/",
            "title": "ML-DL-Dropout怎样缓解RNN过拟合",
            "date_published": "2022-05-13T01:35:27.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-ji-cheng-xue-xi-de-na-xie-shi/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-ji-cheng-xue-xi-de-na-xie-shi/",
            "title": "ML-DL-集成学习的那些事",
            "date_published": "2022-05-13T01:11:56.000Z",
            "content_html": "",
            "tags": [
                "XGBoost",
                "GBDT",
                "梯度提升树",
                "基分类器"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-mo-xing-ping-gu-de-hen-duo-shi/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-mo-xing-ping-gu-de-hen-duo-shi/",
            "title": "ML-DL-模型评估的很多事",
            "date_published": "2022-05-13T01:04:23.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-guan-yu-jiang-wei-wo-xiang-shuo/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-guan-yu-jiang-wei-wo-xiang-shuo/",
            "title": "ML-DL-关于降维我想说",
            "date_published": "2022-05-13T01:02:08.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/13/ml-dl-te-zheng-gong-cheng-de-na-xie-po-shi/",
            "url": "https://www.kitx86.com/2022/05/13/ml-dl-te-zheng-gong-cheng-de-na-xie-po-shi/",
            "title": "ML-Dl-特征工程的那些破事",
            "date_published": "2022-05-13T01:01:03.000Z",
            "content_html": "",
            "tags": []
        },
        {
            "id": "https://www.kitx86.com/2022/05/11/ml-dl-audio-de-na-xie-shi/",
            "url": "https://www.kitx86.com/2022/05/11/ml-dl-audio-de-na-xie-shi/",
            "title": "ML-DL-audio的那些事",
            "date_published": "2022-05-11T00:55:26.000Z",
            "content_html": "<h4 id=\"内篇-amp-amp-齐物论\"><a href=\"#内篇-amp-amp-齐物论\" class=\"headerlink\" title=\"内篇&amp;&amp;齐物论\"></a>内篇&amp;&amp;齐物论</h4><p>源址：<a href=\"https://gitee.com/mindspore/models/tree/master/research/audio\">https://gitee.com/mindspore/models/tree/master/research/audio</a></p>\n<p>下面的几个模型是列在 gitee / audio上面的，我拿来看了前面几个</p>\n<h3 id=\"1-FastSpeech\"><a href=\"#1-FastSpeech\" class=\"headerlink\" title=\"1.FastSpeech\"></a>1.FastSpeech</h3><h4 id=\"描述\"><a href=\"#描述\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>基于神经网络的端到端文本到语音（TTS）显着提高了合成语音的质量。 TTS 方法通常首先从文本生成梅尔谱图，然后使用声码器（如 WaveNet（该工作中的 WaveGlow））从梅尔谱图合成语音。与传统的连接和统计参数方法相比，基于神经网络的端到端模型推理速度慢，合成语音通常鲁棒性不强（即某些单词被跳过或重复）和缺乏可控性（语音速度或韵律控制）。在这项工作中，我们使用基于 Transformer 的前馈网络为 TTS 并行生成梅尔谱图。具体来说，我们使用先前从基于编码器-解码器的教师模型中提取的注意力对齐来进行音素持续时间预测，长度调节器使用该模型来扩展源音素序列以匹配并行梅尔谱图的目标梅尔谱图序列的长度一代。在 LJSpeech 数据集上的实验表明，并行模型在语音质量方面与自回归模型相匹配，在特别困难的情况下几乎消除了跳字和重复的问题，并且可以平滑地调整语音速度。</p>\n<h4 id=\"模型架构\"><a href=\"#模型架构\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>FastSpeech 的架构是基于 Transformer 中的自注意力和一维卷积的前馈结构。这种结构称为前馈变压器（FFT）。 Feed-Forward Transformer 堆叠多个 FFT 块用于音素到 mel 谱图的转换，音素侧有 N 个块，mel 谱图侧有 N 个块，中间有一个长度调节器来弥合音素和 mel 之间的长度差距-频谱图序列。每个 FFT 块由一个自注意力和一维卷积网络组成。自注意力网络由一个多头注意力组成，用于提取交叉位置信息。与 Transformer 中的 2 层密集网络不同，FastSpeech 使用带有 ReLU 激活的 2 层 1D 卷积网络。动机是相邻的隐藏状态在语音任务中的字符/音素和梅尔谱图序列中更密切相关。</p>\n<h3 id=\"2-ctcmodel\"><a href=\"#2-ctcmodel\" class=\"headerlink\" title=\"2.ctcmodel\"></a>2.ctcmodel</h3><h4 id=\"描述-1\"><a href=\"#描述-1\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>CTCModel利用CTC准则训练RNN模型，完成语素标记任务。CTC 的全称是Connectionist Temporal Classification，中文名称是“连接时序分类”，这个方法主要是解决神经网络label 和output 不对齐的问题，其优点是不用强制对齐标签且标签可变长，仅需输入序列和监督标签序列即可进行训练。 CTC被广泛的应用在语音识别，OCR等任务上，取得了显著的效果</p>\n<h4 id=\"模型架构-1\"><a href=\"#模型架构-1\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>一个两层的双向LSTM模型，输入维度为39，即提取出的语音特征的维度<br>一个全连接层，输出维度为62,标签数+1,61代表空白符号</p>\n<h3 id=\"3-deepspeech2\"><a href=\"#3-deepspeech2\" class=\"headerlink\" title=\"3.deepspeech2\"></a>3.deepspeech2</h3><h4 id=\"描述-2\"><a href=\"#描述-2\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>DeepSpeech2是一个使用 CTC 损失训练的语音识别模型。它用神经网络取代了整个手工设计的管道，可以处理各种各样的语音，包括嘈杂的环境、口音和不同的语言。</p>\n<h4 id=\"模型架构-2\"><a href=\"#模型架构-2\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><ul>\n<li>两个卷积层:<ul>\n<li>通道数为 32，内核大小为 41, 11 ，步长为 2, 2</li>\n<li>通道数为 32，内核大小为 41, 11 ，步长为 2, 1</li>\n</ul>\n</li>\n<li>五个双向 LSTM 层（大小为 1024）</li>\n<li>一个投影层【大小为字符数加 1（为CTC空白符号)，29】</li>\n</ul>\n<h3 id=\"4-dscnn\"><a href=\"#4-dscnn\" class=\"headerlink\" title=\"4.dscnn\"></a>4.dscnn</h3><h4 id=\"描述-3\"><a href=\"#描述-3\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>DS-CNN 是深度可分离的卷积神经网络，于 2017 年首次用于关键字识别。KWS 应用具有高度受限的功率预算，通常在内存和计算能力有限的微型微控制器上运行。深度可分离卷积在参数数量和操作方面都更有效，这使得即使在资源受限的微控制器设备中也可以实现更深、更宽的架构。</p>\n<h3 id=\"5-fcn-4\"><a href=\"#5-fcn-4\" class=\"headerlink\" title=\"5.fcn-4\"></a>5.fcn-4</h3><h4 id=\"描述-4\"><a href=\"#描述-4\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>这个存储库提供了一个脚本和配方来训练 FCN-4 模型以实现最先进的准确性。<br>论文：“Keunwoo Choi、George Fazekas 和 Mark Sandler，“使用深度卷积神经网络进行自动标记”，国际音乐协会信息检索会议。ISMIR，2016 年。</p>\n<h4 id=\"模型架构-3\"><a href=\"#模型架构-3\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>FCN-4 是一种卷积神经网络架构，它的名称 FCN-4 来自于它有 4 层的事实。它的层由卷积层、最大池化层、激活层、全连接层组成。</p>\n<h3 id=\"6-speech-transformer\"><a href=\"#6-speech-transformer\" class=\"headerlink\" title=\"6.speech_transformer\"></a>6.speech_transformer</h3><h4 id=\"描述-5\"><a href=\"#描述-5\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>标准的transformer sequence2sequence（编码器，解码器）模型架构用于解决speech2text问题。</p>\n<h4 id=\"模型架构-4\"><a href=\"#模型架构-4\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>具体来说，Transformer 包含六个编码器模块和六个解码器模块。每个编码器模块由一个自注意力层和一个前馈层组成，每个解码器模块由一个自注意力层、一个编码器-解码器-注意力层和一个前馈层组成。</p>\n<h3 id=\"7-tacotron2\"><a href=\"#7-tacotron2\" class=\"headerlink\" title=\"7.tacotron2\"></a>7.tacotron2</h3><h4 id=\"描述-6\"><a href=\"#描述-6\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>Tacotron2 是一款 TTS 机型。它包含两个阶段，在第一阶段它使用序列到序列方法从文本序列中预测梅尔频谱图，在第二阶段它应用 WaveNet 作为声码器将梅尔频谱图转换为波形。我们支持在 Ascend 平台上训练和评估 tacotron2 模型。<br>论文：乔纳森等人。通过在梅尔谱图预测上调节 WaveNet 进行自然 TTS 合成。</p>\n<h4 id=\"模型架构-5\"><a href=\"#模型架构-5\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>Tacotron2本质上是一个包含编码器和解码器的序列到序列模型，编码器由三个conv层和一个BiLSTM层实现，解码器使用两个LSTM层来解码下一个状态，在编码器之间应用位置感知注意力和解码器，然后将解码后的状态输入到由五个卷积层实现的 postnet 来预测梅尔谱图，最后将预测的梅尔谱图特征输入到 WaveNet 声码器以合成语音信号。</p>\n<h3 id=\"8-wavenet\"><a href=\"#8-wavenet\" class=\"headerlink\" title=\"8.wavenet\"></a>8.wavenet</h3><h4 id=\"描述-7\"><a href=\"#描述-7\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>WaveNet 是一种用于生成原始音频波形的深度神经网络。该模型是完全概率和自回归的，每个音频样本的预测分布都以所有先前的样本为条件。我们支持在 Ascend、GPU 和 CPU 上进行训练和评估。</p>\n<h4 id=\"模型架构-6\"><a href=\"#模型架构-6\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>当前模型包括一个预卷积层，然后是几个残差块，这些残差块具有与门控激活单元的残差和跳跃连接。最后，添加后卷积层来预测分布。</p>\n",
            "tags": [
                "语音处理",
                "model"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/09/ml-dl-xun-huan-shen-jing-wang-luo-ji-chu/",
            "url": "https://www.kitx86.com/2022/05/09/ml-dl-xun-huan-shen-jing-wang-luo-ji-chu/",
            "title": "ML-DL-循环神经网络基础",
            "date_published": "2022-05-09T15:05:28.000Z",
            "content_html": "<h2 id=\"\"><a href=\"#\" class=\"headerlink\" title=\"\"></a></h2><p>模型的演变：</p>\n<p>自回归—-隐变量自回归—-</p>\n<p>一元语法，二元语法，三元语法——词元频率</p>\n<p>困惑度</p>\n",
            "tags": [
                "RNN",
                "循环神经网络"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/09/ml-dl-ji-suan-ji-ting-jue-yi-wen/",
            "url": "https://www.kitx86.com/2022/05/09/ml-dl-ji-suan-ji-ting-jue-yi-wen/",
            "title": "ML-DL-计算机听觉疑问",
            "date_published": "2022-05-09T13:07:56.000Z",
            "content_html": "<p>1.音频信息识别常用的数据集有哪些？</p>\n<p>2.音频信息提取特征中常用的梅尔频率倒谱系数的计算</p>\n<p>3.常见的音频识别算法</p>\n",
            "tags": [
                "深度学习",
                "CH"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-yi-wen/",
            "url": "https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-yi-wen/",
            "title": "ML-Dl-自然语言处理疑问",
            "date_published": "2022-05-09T12:49:54.000Z",
            "content_html": "<p>1.机器翻译任务中如何解决未登录词的翻译问题？</p>\n<p>2.解决翻译任务中双语料不足的方法？</p>\n<p>3.如何使用 CNN 和 RNN 来解决 问题系统任务中的长距离依赖问题？ 对比 Transformer有什么好的新意？</p>\n<p>4.在文本段落编码是如何结合问题信息？好处是什么？</p>\n<p>5.如何对文本中词的位置信息进行编码？</p>\n<p>6.语言模型的任务形式是什么？语言模型如何提升其它自然语言处理任务的效果？</p>\n<p>7.常见的词嵌入模型？它们之间联系和区别？</p>\n",
            "tags": [
                "深度学习",
                "NLP"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-ji-chu/",
            "url": "https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-ji-chu/",
            "title": "ML-DL-自然语言处理基础",
            "date_published": "2022-05-09T12:48:15.000Z",
            "content_html": "",
            "tags": [
                "深度学习",
                "NLP"
            ]
        }
    ]
}