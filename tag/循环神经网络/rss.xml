<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>Supernova • Posts by &#34;循环神经网络&#34; tag</title>
        <link>https://www.kitx86.com</link>
        <description>小派过气画家</description>
        <language>en</language>
        <pubDate>Sat, 02 Apr 2022 23:20:41 +0800</pubDate>
        <lastBuildDate>Sat, 02 Apr 2022 23:20:41 +0800</lastBuildDate>
        <category>Personal</category>
        <category>kaggle</category>
        <category>ML</category>
        <category>pytorch</category>
        <category>深度学习</category>
        <category>机器学习</category>
        <category>竞赛模板</category>
        <category>常用tricks</category>
        <category>paper</category>
        <category>deep learning</category>
        <category>neural network</category>
        <category>SGD</category>
        <category>RMSProp</category>
        <category>Adam</category>
        <category>常见数据处理</category>
        <category>CNN</category>
        <category>RNN</category>
        <category>循环神经网络</category>
        <category>data vistualtion</category>
        <category>游戏制作</category>
        <category>音游跑酷</category>
        <category>独立自制</category>
        <category>Willing</category>
        <category>论文写作</category>
        <category>MSRA</category>
        <category>微软亚洲研究院</category>
        <category>QCQI</category>
        <category>data</category>
        <item>
            <guid isPermalink="true">https://www.kitx86.com/2022/04/02/ml-dl-zhu-yi-li-ji-zhi/</guid>
            <title>ML-DL-注意力机制</title>
            <link>https://www.kitx86.com/2022/04/02/ml-dl-zhu-yi-li-ji-zhi/</link>
            <category>深度学习</category>
            <category>循环神经网络</category>
            <pubDate>Sat, 02 Apr 2022 23:20:41 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;注意力机制-Attention-Mechanism&#34;&gt;&lt;a href=&#34;#注意力机制-Attention-Mechanism&#34; class=&#34;headerlink&#34; title=&#34;注意力机制 - Attention Mechanism&#34;&gt;&lt;/a&gt;注意力机制 - Attention Mechanism&lt;/h2&gt;&lt;h3 id=&#34;一、attention-这项工作的出现&#34;&gt;&lt;a href=&#34;#一、attention-这项工作的出现&#34; class=&#34;headerlink&#34; title=&#34;一、attention 这项工作的出现&#34;&gt;&lt;/a&gt;一、attention 这项工作的出现&lt;/h3&gt;&lt;h3 id=&#34;二、针对注意力机制所包含的一些区分&#34;&gt;&lt;a href=&#34;#二、针对注意力机制所包含的一些区分&#34; class=&#34;headerlink&#34; title=&#34;二、针对注意力机制所包含的一些区分&#34;&gt;&lt;/a&gt;二、针对注意力机制所包含的一些区分&lt;/h3&gt;&lt;p&gt;1、软注意，侧重于吸取的信息相对来说会偏向全面&lt;/p&gt;
&lt;p&gt;2、硬注意，会直接舍弃部分信息来保留计算的轻便&lt;/p&gt;
&lt;p&gt;3、自注意机制，由输入权重来相互作用，其实这样的状况是由内部来决定，&lt;/p&gt;
&lt;p&gt;预训练模型看起来和 Transformer 有着很大的干系，可以说是语言模型的核心网络，&lt;/p&gt;
&lt;p&gt;transformer 在我这里给出的解释是，一段话作为输入，首先将输入中的各个词在self-attention 机制下变成词向量，同时会含有一连串的每一个词的位置向量 （ 不行，我完全是跟着别人走的）&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://www.kitx86.com/2022/03/28/ml-dl-xun-huan-shen-jing-wang-luo/</guid>
            <title>ML-DL-循环神经网络</title>
            <link>https://www.kitx86.com/2022/03/28/ml-dl-xun-huan-shen-jing-wang-luo/</link>
            <category>RNN</category>
            <category>循环神经网络</category>
            <pubDate>Mon, 28 Mar 2022 14:37:08 +0800</pubDate>
            <description><![CDATA[  ]]></description>
        </item>
    </channel>
</rss>
