<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>https://www.kitx86.com</id>
    <title>Supernova • Posts by &#34;循环神经网络&#34; tag</title>
    <link href="https://www.kitx86.com" />
    <updated>2022-04-02T15:20:41.000Z</updated>
    <category term="日记" />
    <category term="Personal" />
    <category term="kaggle" />
    <category term="ML" />
    <category term="竞赛模板" />
    <category term="常用tricks" />
    <category term="pytorch" />
    <category term="深度学习" />
    <category term="机器学习" />
    <category term="paper" />
    <category term="deep learning" />
    <category term="neural network" />
    <category term="SGD" />
    <category term="RMSProp" />
    <category term="Adam" />
    <category term="CNN" />
    <category term="常见数据处理" />
    <category term="循环神经网络" />
    <category term="data vistualtion" />
    <category term="RNN" />
    <category term="Willing" />
    <category term="游戏制作" />
    <category term="音游跑酷" />
    <category term="独立自制" />
    <category term="MSRA" />
    <category term="微软亚洲研究院" />
    <category term="微不足道的贡献" />
    <category term="论文写作" />
    <category term="荒诞的故事" />
    <category term="饲养记" />
    <category term="QCQI" />
    <category term="data" />
    <entry>
        <id>https://www.kitx86.com/2022/04/02/ml-dl-zhu-yi-li-ji-zhi/</id>
        <title>ML-DL-注意力机制</title>
        <link rel="alternate" href="https://www.kitx86.com/2022/04/02/ml-dl-zhu-yi-li-ji-zhi/"/>
        <content type="html">&lt;h2 id=&#34;注意力机制-Attention-Mechanism&#34;&gt;&lt;a href=&#34;#注意力机制-Attention-Mechanism&#34; class=&#34;headerlink&#34; title=&#34;注意力机制 - Attention Mechanism&#34;&gt;&lt;/a&gt;注意力机制 - Attention Mechanism&lt;/h2&gt;&lt;h3 id=&#34;一、attention-这项工作的出现&#34;&gt;&lt;a href=&#34;#一、attention-这项工作的出现&#34; class=&#34;headerlink&#34; title=&#34;一、attention 这项工作的出现&#34;&gt;&lt;/a&gt;一、attention 这项工作的出现&lt;/h3&gt;&lt;h3 id=&#34;二、针对注意力机制所包含的一些区分&#34;&gt;&lt;a href=&#34;#二、针对注意力机制所包含的一些区分&#34; class=&#34;headerlink&#34; title=&#34;二、针对注意力机制所包含的一些区分&#34;&gt;&lt;/a&gt;二、针对注意力机制所包含的一些区分&lt;/h3&gt;&lt;p&gt;1、软注意，侧重于吸取的信息相对来说会偏向全面&lt;/p&gt;
&lt;p&gt;2、硬注意，会直接舍弃部分信息来保留计算的轻便&lt;/p&gt;
&lt;p&gt;3、自注意机制，由输入权重来相互作用，其实这样的状况是由内部来决定，&lt;/p&gt;
&lt;p&gt;预训练模型看起来和 Transformer 有着很大的干系，可以说是语言模型的核心网络，&lt;/p&gt;
&lt;p&gt;transformer 在我这里给出的解释是，一段话作为输入，首先将输入中的各个词在self-attention 机制下变成词向量，同时会含有一连串的每一个词的位置向量 （ 不行，我完全是跟着别人走的）&lt;/p&gt;
</content>
        <category term="深度学习" />
        <category term="循环神经网络" />
        <updated>2022-04-02T15:20:41.000Z</updated>
    </entry>
    <entry>
        <id>https://www.kitx86.com/2022/03/28/ml-dl-xun-huan-shen-jing-wang-luo/</id>
        <title>ML-DL-循环神经网络</title>
        <link rel="alternate" href="https://www.kitx86.com/2022/03/28/ml-dl-xun-huan-shen-jing-wang-luo/"/>
        <content type="html"></content>
        <category term="循环神经网络" />
        <category term="RNN" />
        <updated>2022-03-28T06:37:08.000Z</updated>
    </entry>
</feed>
