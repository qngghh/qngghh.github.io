<?xml version="1.0"?>
<rss version="2.0">
    <channel>
        <title>Supernova • Posts by &#34;深度学习&#34; tag</title>
        <link>https://www.kitx86.com</link>
        <description>小派过气画家</description>
        <language>en</language>
        <pubDate>Mon, 09 May 2022 21:07:56 +0800</pubDate>
        <lastBuildDate>Mon, 09 May 2022 21:07:56 +0800</lastBuildDate>
        <category>日记</category>
        <category>生物学</category>
        <category>kaggle</category>
        <category>ML</category>
        <category>DL</category>
        <category>竞赛模板</category>
        <category>常用tricks</category>
        <category>paper</category>
        <category>deep learning</category>
        <category>neural network</category>
        <category>Personal</category>
        <category>机器学习</category>
        <category>SGD</category>
        <category>RMSProp</category>
        <category>Adam</category>
        <category>CNN</category>
        <category>卷积神经网络</category>
        <category>深度学习</category>
        <category>RNN</category>
        <category>循环神经网络</category>
        <category>NLP</category>
        <category>常见数据处理</category>
        <category>CH</category>
        <category>data vistualtion</category>
        <category>Noince</category>
        <category>advance</category>
        <category>Willing</category>
        <category>荒诞的故事</category>
        <category>游戏制作</category>
        <category>音游跑酷</category>
        <category>独立自制</category>
        <category>微不足道的贡献</category>
        <category>MSRA</category>
        <category>微软亚洲研究院</category>
        <category>论文写作</category>
        <category>饲养记</category>
        <category>QCQI</category>
        <category>data</category>
        <item>
            <guid isPermalink="true">https://www.kitx86.com/2022/05/09/ml-dl-ji-suan-ji-ting-jue-yi-wen/</guid>
            <title>ML-DL-计算机听觉疑问</title>
            <link>https://www.kitx86.com/2022/05/09/ml-dl-ji-suan-ji-ting-jue-yi-wen/</link>
            <category>深度学习</category>
            <category>CH</category>
            <pubDate>Mon, 09 May 2022 21:07:56 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;1.音频信息识别常用的数据集有哪些？&lt;/p&gt;
&lt;p&gt;2.音频信息提取特征中常用的梅尔频率倒谱系数的计算&lt;/p&gt;
&lt;p&gt;3.常见的音频识别算法&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-yi-wen/</guid>
            <title>ML-Dl-自然语言处理疑问</title>
            <link>https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-yi-wen/</link>
            <category>深度学习</category>
            <category>NLP</category>
            <pubDate>Mon, 09 May 2022 20:49:54 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;1.机器翻译任务中如何解决未登录词的翻译问题？&lt;/p&gt;
&lt;p&gt;2.解决翻译任务中双语料不足的方法？&lt;/p&gt;
&lt;p&gt;3.如何使用 CNN 和 RNN 来解决 问题系统任务中的长距离依赖问题？ 对比 Transformer有什么好的新意？&lt;/p&gt;
&lt;p&gt;4.在文本段落编码是如何结合问题信息？好处是什么？&lt;/p&gt;
&lt;p&gt;5.如何对文本中词的位置信息进行编码？&lt;/p&gt;
&lt;p&gt;6.语言模型的任务形式是什么？语言模型如何提升其它自然语言处理任务的效果？&lt;/p&gt;
&lt;p&gt;7.常见的词嵌入模型？它们之间联系和区别？&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-ji-chu/</guid>
            <title>ML-DL-自然语言处理基础</title>
            <link>https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-ji-chu/</link>
            <category>深度学习</category>
            <category>NLP</category>
            <pubDate>Mon, 09 May 2022 20:48:15 +0800</pubDate>
            <description><![CDATA[  ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://www.kitx86.com/2022/05/09/ml-dl-qian-xiang-shen-jing-wang-luo-ji-chu/</guid>
            <title>ML-DL-前向神经网络基础</title>
            <link>https://www.kitx86.com/2022/05/09/ml-dl-qian-xiang-shen-jing-wang-luo-ji-chu/</link>
            <category>深度学习</category>
            <pubDate>Mon, 09 May 2022 20:28:36 +0800</pubDate>
            <description><![CDATA[  ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://www.kitx86.com/2022/05/09/ml-dl-xun-huan-shen-jing-wang-luo-zhong-de-bu-fen-wen-ti/</guid>
            <title>ML-DL-循环神经网络中的部分问题</title>
            <link>https://www.kitx86.com/2022/05/09/ml-dl-xun-huan-shen-jing-wang-luo-zhong-de-bu-fen-wen-ti/</link>
            <category>深度学习</category>
            <pubDate>Mon, 09 May 2022 19:44:48 +0800</pubDate>
            <description><![CDATA[ &lt;p&gt;1.RNN 的结构及参数更新方式？&lt;/p&gt;
&lt;p&gt;2.使用 CNN 对序列数据建模？&lt;/p&gt;
&lt;p&gt;3.dropout能缓解过拟合？&lt;/p&gt;
&lt;p&gt;4.RNN 怎样去使用 dropout？&lt;/p&gt;
&lt;p&gt;5.RNN 中出现的长期依赖状况？&lt;/p&gt;
&lt;p&gt;6.针对长期以来问题在 RNN 实现 LSTM 功能？&lt;/p&gt;
&lt;p&gt;7.GRU 用两个门控单元就控制了时间序列的记忆及遗忘行为？ &lt;/p&gt;
&lt;p&gt;8.用RNN来实现 Seq2Seq 映射&lt;/p&gt;
&lt;p&gt;9.Seq2Seq 在编码-解码中会存在信息丢失？有什么好的解决方案&lt;/p&gt;
 ]]></description>
        </item>
        <item>
            <guid isPermalink="true">https://www.kitx86.com/2022/04/02/ml-dl-zhu-yi-li-ji-zhi/</guid>
            <title>ML-DL-注意力机制</title>
            <link>https://www.kitx86.com/2022/04/02/ml-dl-zhu-yi-li-ji-zhi/</link>
            <category>深度学习</category>
            <category>循环神经网络</category>
            <pubDate>Sat, 02 Apr 2022 23:20:41 +0800</pubDate>
            <description><![CDATA[ &lt;h2 id=&#34;注意力机制-Attention-Mechanism&#34;&gt;&lt;a href=&#34;#注意力机制-Attention-Mechanism&#34; class=&#34;headerlink&#34; title=&#34;注意力机制 - Attention Mechanism&#34;&gt;&lt;/a&gt;注意力机制 - Attention Mechanism&lt;/h2&gt;&lt;h3 id=&#34;一、attention-这项工作的出现&#34;&gt;&lt;a href=&#34;#一、attention-这项工作的出现&#34; class=&#34;headerlink&#34; title=&#34;一、attention 这项工作的出现&#34;&gt;&lt;/a&gt;一、attention 这项工作的出现&lt;/h3&gt;&lt;h3 id=&#34;二、针对注意力机制所包含的一些区分&#34;&gt;&lt;a href=&#34;#二、针对注意力机制所包含的一些区分&#34; class=&#34;headerlink&#34; title=&#34;二、针对注意力机制所包含的一些区分&#34;&gt;&lt;/a&gt;二、针对注意力机制所包含的一些区分&lt;/h3&gt;&lt;p&gt;1、软注意，侧重于吸取的信息相对来说会偏向全面&lt;/p&gt;
&lt;p&gt;2、硬注意，会直接舍弃部分信息来保留计算的轻便&lt;/p&gt;
&lt;p&gt;3、自注意机制，由输入权重来相互作用，其实这样的状况是由内部来决定，&lt;/p&gt;
&lt;p&gt;预训练模型看起来和 Transformer 有着很大的干系，可以说是语言模型的核心网络，&lt;/p&gt;
&lt;p&gt;transformer 在我这里给出的解释是，一段话作为输入，首先将输入中的各个词在self-attention 机制下变成词向量，同时会含有一连串的每一个词的位置向量 （ 不行，我完全是跟着别人走的）&lt;/p&gt;
 ]]></description>
        </item>
    </channel>
</rss>
