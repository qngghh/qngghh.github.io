{
    "version": "https://jsonfeed.org/version/1",
    "title": "Supernova • All posts by \"深度学习\" tag",
    "description": "小派过气画家",
    "home_page_url": "https://www.kitx86.com",
    "items": [
        {
            "id": "https://www.kitx86.com/2022/05/09/ml-dl-ji-suan-ji-ting-jue-yi-wen/",
            "url": "https://www.kitx86.com/2022/05/09/ml-dl-ji-suan-ji-ting-jue-yi-wen/",
            "title": "ML-DL-计算机听觉疑问",
            "date_published": "2022-05-09T13:07:56.000Z",
            "content_html": "<p>1.音频信息识别常用的数据集有哪些？</p>\n<p>2.音频信息提取特征中常用的梅尔频率倒谱系数的计算</p>\n<p>3.常见的音频识别算法</p>\n",
            "tags": [
                "深度学习",
                "CH"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-yi-wen/",
            "url": "https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-yi-wen/",
            "title": "ML-Dl-自然语言处理疑问",
            "date_published": "2022-05-09T12:49:54.000Z",
            "content_html": "<p>1.机器翻译任务中如何解决未登录词的翻译问题？</p>\n<p>2.解决翻译任务中双语料不足的方法？</p>\n<p>3.如何使用 CNN 和 RNN 来解决 问题系统任务中的长距离依赖问题？ 对比 Transformer有什么好的新意？</p>\n<p>4.在文本段落编码是如何结合问题信息？好处是什么？</p>\n<p>5.如何对文本中词的位置信息进行编码？</p>\n<p>6.语言模型的任务形式是什么？语言模型如何提升其它自然语言处理任务的效果？</p>\n<p>7.常见的词嵌入模型？它们之间联系和区别？</p>\n",
            "tags": [
                "深度学习",
                "NLP"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-ji-chu/",
            "url": "https://www.kitx86.com/2022/05/09/ml-dl-zi-ran-yu-yan-chu-li-ji-chu/",
            "title": "ML-DL-自然语言处理基础",
            "date_published": "2022-05-09T12:48:15.000Z",
            "content_html": "",
            "tags": [
                "深度学习",
                "NLP"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/09/ml-dl-qian-xiang-shen-jing-wang-luo-ji-chu/",
            "url": "https://www.kitx86.com/2022/05/09/ml-dl-qian-xiang-shen-jing-wang-luo-ji-chu/",
            "title": "ML-DL-前向神经网络基础",
            "date_published": "2022-05-09T12:28:36.000Z",
            "content_html": "",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/05/09/ml-dl-xun-huan-shen-jing-wang-luo-zhong-de-bu-fen-wen-ti/",
            "url": "https://www.kitx86.com/2022/05/09/ml-dl-xun-huan-shen-jing-wang-luo-zhong-de-bu-fen-wen-ti/",
            "title": "ML-DL-循环神经网络中的部分问题",
            "date_published": "2022-05-09T11:44:48.000Z",
            "content_html": "<p>1.RNN 的结构及参数更新方式？</p>\n<p>2.使用 CNN 对序列数据建模？</p>\n<p>3.dropout能缓解过拟合？</p>\n<p>4.RNN 怎样去使用 dropout？</p>\n<p>5.RNN 中出现的长期依赖状况？</p>\n<p>6.针对长期以来问题在 RNN 实现 LSTM 功能？</p>\n<p>7.GRU 用两个门控单元就控制了时间序列的记忆及遗忘行为？ </p>\n<p>8.用RNN来实现 Seq2Seq 映射</p>\n<p>9.Seq2Seq 在编码-解码中会存在信息丢失？有什么好的解决方案</p>\n",
            "tags": [
                "深度学习"
            ]
        },
        {
            "id": "https://www.kitx86.com/2022/04/02/ml-dl-zhu-yi-li-ji-zhi/",
            "url": "https://www.kitx86.com/2022/04/02/ml-dl-zhu-yi-li-ji-zhi/",
            "title": "ML-DL-注意力机制",
            "date_published": "2022-04-02T15:20:41.000Z",
            "content_html": "<h2 id=\"注意力机制-Attention-Mechanism\"><a href=\"#注意力机制-Attention-Mechanism\" class=\"headerlink\" title=\"注意力机制 - Attention Mechanism\"></a>注意力机制 - Attention Mechanism</h2><h3 id=\"一、attention-这项工作的出现\"><a href=\"#一、attention-这项工作的出现\" class=\"headerlink\" title=\"一、attention 这项工作的出现\"></a>一、attention 这项工作的出现</h3><h3 id=\"二、针对注意力机制所包含的一些区分\"><a href=\"#二、针对注意力机制所包含的一些区分\" class=\"headerlink\" title=\"二、针对注意力机制所包含的一些区分\"></a>二、针对注意力机制所包含的一些区分</h3><p>1、软注意，侧重于吸取的信息相对来说会偏向全面</p>\n<p>2、硬注意，会直接舍弃部分信息来保留计算的轻便</p>\n<p>3、自注意机制，由输入权重来相互作用，其实这样的状况是由内部来决定，</p>\n<p>预训练模型看起来和 Transformer 有着很大的干系，可以说是语言模型的核心网络，</p>\n<p>transformer 在我这里给出的解释是，一段话作为输入，首先将输入中的各个词在self-attention 机制下变成词向量，同时会含有一连串的每一个词的位置向量 （ 不行，我完全是跟着别人走的）</p>\n",
            "tags": [
                "深度学习",
                "循环神经网络"
            ]
        }
    ]
}