{
    "version": "https://jsonfeed.org/version/1",
    "title": "Supernova • All posts by \"cnn\" tag",
    "description": "小派过气画家",
    "home_page_url": "https://www.kitx86.com",
    "items": [
        {
            "id": "https://www.kitx86.com/2022/03/26/ml-dl-juan-ji-shen-jing-wang-luo/",
            "url": "https://www.kitx86.com/2022/03/26/ml-dl-juan-ji-shen-jing-wang-luo/",
            "title": "ML-DL-卷积神经网络",
            "date_published": "2022-03-26T07:28:39.000Z",
            "content_html": "<h3 id=\"一、卷积神经网络的诞生\"><a href=\"#一、卷积神经网络的诞生\" class=\"headerlink\" title=\"一、卷积神经网络的诞生\"></a>一、卷积神经网络的诞生</h3><p>1.对该类型网络的研究都源于其他人的，一样的内容就不在过多的赘述：[图像学的内容不太感兴趣]</p>\n<p>主要用途：处理图像数据、表格数据等等</p>\n<p><u>竞赛可选择：图像识别，目标检测-边缘检测，语义分割</u></p>\n<p>在对 DNN 的架构添加了 卷积 layer 后就是 CNN ，这就是 CNN 的由来，但 卷积的操作方式有些奇怪的行为让人大跌眼镜</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token comment\"># 构造一个二维卷积层，它具有1个输出通道和形状为（1，2）的卷积核</span>\n\n<span class=\"token comment\"># 二维卷积核 参数形式 (批量大小，通道，高度，宽度)</span>\nconv2d <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> bias<span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">)</span>\n<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>特性：空间不变性</p>\n<ol>\n<li><em>平移不变性</em>（translation invariance）：不管检测对象出现在图像中的哪个位置，神经网络的前面几层应该对相同的图像区域具有相似的反应，即为“平移不变性”。</li>\n<li><em>局部性</em>（locality）：神经网络的前面几层应该只探索输入图像中的局部区域，而不过度在意图像中相隔较远区域的关系，这就是“局部性”原则。最终，可以聚合这些局部特征，以在整个图像级别进行预测。</li>\n</ol>\n<p>输出的卷积层 &gt; 特征映射</p>\n<p>感受野：指前向传播中可能影响 x 计算的所有元素</p>\n<p>2、卷积神经网络之LeNet</p>\n<p>LeNet（LeNet-5）由两个部分组成：</p>\n<ul>\n<li>卷积编码器：由两个卷积层组成;</li>\n<li>全连接层密集块：由三个全连接层组成。</li>\n</ul>\n<p><img src=\"/ML-Dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/lenet.svg\" alt=\"lenet\"></p>\n<h4 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h4><ul>\n<li>卷积神经网络（CNN）是一类使用卷积层的网络。</li>\n<li>在卷积神经网络中，我们组合使用卷积层、非线性激活函数和汇聚层。</li>\n<li>为了构造高性能的卷积神经网络，我们通常对卷积层进行排列，逐渐降低其表示的空间分辨率，同时增加通道数。</li>\n<li>在传统的卷积神经网络中，卷积块编码得到的表征在输出之前需由一个或多个全连接层进行处理。</li>\n<li>LeNet是最早发布的卷积神经网络之一。</li>\n</ul>\n<h4 id=\"考查\"><a href=\"#考查\" class=\"headerlink\" title=\"考查\"></a>考查</h4><ol>\n<li><p>将平均汇聚层替换为最大汇聚层，会发生什么？</p>\n</li>\n<li><p>尝试构建一个基于LeNet的更复杂的网络，以提高其准确性。</p>\n<ol>\n<li>调整卷积窗口大小。</li>\n<li>调整输出通道的数量。</li>\n<li>调整激活函数（如ReLU）。</li>\n<li>调整卷积层的数量。</li>\n<li>调整全连接层的数量。</li>\n<li>调整学习率和其他训练细节（例如，初始化和轮数）。</li>\n</ol>\n</li>\n<li><p>在MNIST数据集上尝试以上改进的网络。</p>\n</li>\n<li><p>显示不同输入（例如毛衣和外套）时，LeNet第一层和第二层的激活值。</p>\n</li>\n</ol>\n<h2 id=\"二、现代卷积神经网络\"><a href=\"#二、现代卷积神经网络\" class=\"headerlink\" title=\"二、现代卷积神经网络\"></a>二、现代卷积神经网络</h2><h3 id=\"1、AlexNet\"><a href=\"#1、AlexNet\" class=\"headerlink\" title=\"1、AlexNet\"></a>1、AlexNet</h3><ul>\n<li><p>AlexNet。它是第一个在大规模视觉竞赛中击败传统计算机视觉模型的大型神经网络；</p>\n</li>\n<li><p>使用重复块的网络（VGG）。它利用许多重复的神经网络块；</p>\n</li>\n<li><p>网络中的网络（NiN）。它重复使用由卷积层和1×1卷积层（用来代替全连接层）来构建深层网络;</p>\n</li>\n<li><p>含并行连结的网络（GoogLeNet）。它使用并行连结的网络，通过不同窗口大小的卷积层和最大汇聚层来并行抽取信息；</p>\n</li>\n<li><p>残差网络（ResNet）。它通过残差块构建跨层的数据通道，是计算机视觉中最流行的体系架构；</p>\n</li>\n<li><p>稠密连接网络（DenseNet）。它的计算成本很高，但给我们带来了更好的效果。</p>\n</li>\n</ul>\n<p>学习表征观摩图</p>\n<p><img src=\"/ML-Dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/filters.png\" alt=\"filters\"></p>\n<p><img src=\"/ML-Dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/alexnet.svg\" alt=\"alexnet\"></p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\"><span class=\"token keyword\">import</span> torch\n<span class=\"token keyword\">from</span> torch <span class=\"token keyword\">import</span> nn\n<span class=\"token keyword\">from</span> d2l <span class=\"token keyword\">import</span> torch <span class=\"token keyword\">as</span> d2l\n\nnet <span class=\"token operator\">=</span> nn<span class=\"token punctuation\">.</span>Sequential<span class=\"token punctuation\">(</span>\n    <span class=\"token comment\"># 这里，我们使用一个11*11的更大窗口来捕捉对象。</span>\n    <span class=\"token comment\"># 同时，步幅为4，以减少输出的高度和宽度。</span>\n    <span class=\"token comment\"># 另外，输出通道的数目远大于LeNet</span>\n    nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">96</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">11</span><span class=\"token punctuation\">,</span> stride<span class=\"token operator\">=</span><span class=\"token number\">4</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>MaxPool2d<span class=\"token punctuation\">(</span>kernel_size<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> stride<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span>\n    nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span><span class=\"token number\">96</span><span class=\"token punctuation\">,</span> <span class=\"token number\">256</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">5</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>MaxPool2d<span class=\"token punctuation\">(</span>kernel_size<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> stride<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 使用三个连续的卷积层和较小的卷积窗口。</span>\n    <span class=\"token comment\"># 除了最后的卷积层，输出通道的数量进一步增加。</span>\n    <span class=\"token comment\"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span>\n    nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span><span class=\"token number\">256</span><span class=\"token punctuation\">,</span> <span class=\"token number\">384</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span><span class=\"token number\">384</span><span class=\"token punctuation\">,</span> <span class=\"token number\">384</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>Conv2d<span class=\"token punctuation\">(</span><span class=\"token number\">384</span><span class=\"token punctuation\">,</span> <span class=\"token number\">256</span><span class=\"token punctuation\">,</span> kernel_size<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> padding<span class=\"token operator\">=</span><span class=\"token number\">1</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>MaxPool2d<span class=\"token punctuation\">(</span>kernel_size<span class=\"token operator\">=</span><span class=\"token number\">3</span><span class=\"token punctuation\">,</span> stride<span class=\"token operator\">=</span><span class=\"token number\">2</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>Flatten<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span>\n    nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">6400</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4096</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span>p<span class=\"token operator\">=</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">4096</span><span class=\"token punctuation\">,</span> <span class=\"token number\">4096</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span> nn<span class=\"token punctuation\">.</span>ReLU<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    nn<span class=\"token punctuation\">.</span>Dropout<span class=\"token punctuation\">(</span>p<span class=\"token operator\">=</span><span class=\"token number\">0.5</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>\n    <span class=\"token comment\"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span>\n    nn<span class=\"token punctuation\">.</span>Linear<span class=\"token punctuation\">(</span><span class=\"token number\">4096</span><span class=\"token punctuation\">,</span> <span class=\"token number\">10</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n<p>观察网络每一层输出架构</p>\n<pre class=\"line-numbers language-python\" data-language=\"python\"><code class=\"language-python\">X <span class=\"token operator\">=</span> torch<span class=\"token punctuation\">.</span>randn<span class=\"token punctuation\">(</span><span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span> <span class=\"token number\">224</span><span class=\"token punctuation\">,</span> <span class=\"token number\">224</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">for</span> layer <span class=\"token keyword\">in</span> net<span class=\"token punctuation\">:</span>\n    X<span class=\"token operator\">=</span>layer<span class=\"token punctuation\">(</span>X<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span>layer<span class=\"token punctuation\">.</span>__class__<span class=\"token punctuation\">.</span>__name__<span class=\"token punctuation\">,</span><span class=\"token string\">'output shape:\\t'</span><span class=\"token punctuation\">,</span>X<span class=\"token punctuation\">.</span>shape<span class=\"token punctuation\">)</span><span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span></span></code></pre>\n\n<pre class=\"line-numbers language-markdown\" data-language=\"markdown\"><code class=\"language-markdown\">Conv2d output shape:\t torch.Size([1, 96, 54, 54])\nReLU output shape:\t torch.Size([1, 96, 54, 54])\nMaxPool2d output shape:\t torch.Size([1, 96, 26, 26])\nConv2d output shape:\t torch.Size([1, 256, 26, 26])\nReLU output shape:\t torch.Size([1, 256, 26, 26])\nMaxPool2d output shape:\t torch.Size([1, 256, 12, 12])\nConv2d output shape:\t torch.Size([1, 384, 12, 12])\nReLU output shape:\t torch.Size([1, 384, 12, 12])\nConv2d output shape:\t torch.Size([1, 384, 12, 12])\nReLU output shape:\t torch.Size([1, 384, 12, 12])\nConv2d output shape:\t torch.Size([1, 256, 12, 12])\nReLU output shape:\t torch.Size([1, 256, 12, 12])\nMaxPool2d output shape:\t torch.Size([1, 256, 5, 5])\nFlatten output shape:\t torch.Size([1, 6400])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 4096])\nReLU output shape:\t torch.Size([1, 4096])\nDropout output shape:\t torch.Size([1, 4096])\nLinear output shape:\t torch.Size([1, 10])<span aria-hidden=\"true\" class=\"line-numbers-rows\"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>\n\n\n\n<p>使用更小的学习速率训练，这是因为网络更深更广、图像分辨率更高，训练卷积神经网络就更昂贵。</p>\n<h4 id=\"小结-1\"><a href=\"#小结-1\" class=\"headerlink\" title=\"小结\"></a>小结</h4><ul>\n<li>AlexNet的架构与LeNet相似，但使用了更多的卷积层和更多的参数来拟合大规模的ImageNet数据集。</li>\n<li>今天，AlexNet已经被更有效的架构所超越，但它是从浅层网络到深层网络的关键一步。</li>\n<li>尽管AlexNet的代码只比LeNet多出几行，但学术界花了很多年才接受深度学习这一概念，并应用其出色的实验结果。这也是由于缺乏有效的计算工具。</li>\n<li>Dropout、ReLU和预处理是提升计算机视觉任务性能的其他关键步骤。</li>\n</ul>\n<h4 id=\"练习\"><a href=\"#练习\" class=\"headerlink\" title=\"练习\"></a>练习</h4><ol>\n<li>试着增加迭代轮数。对比LeNet的结果有什么不同？为什么？</li>\n<li>AlexNet对于Fashion-MNIST数据集来说可能太复杂了。<ol>\n<li>尝试简化模型以加快训练速度，同时确保准确性不会显著下降。</li>\n<li>设计一个更好的模型，可以直接在28×28图像上工作。</li>\n</ol>\n</li>\n<li>修改批量大小，并观察模型精度和GPU显存变化。</li>\n<li>分析了AlexNet的计算性能。<ol>\n<li>在AlexNet中主要是哪部分占用显存？</li>\n<li>在AlexNet中主要是哪部分需要更多的计算？</li>\n<li>计算结果时显存带宽如何？</li>\n</ol>\n</li>\n<li>将dropout和ReLU应用于LeNet-5，效果有提升吗？再试试预处理会怎么样？</li>\n</ol>\n<h3 id=\"2、使用块的网络：-VGG-和AlexNet-没有很大的差别\"><a href=\"#2、使用块的网络：-VGG-和AlexNet-没有很大的差别\" class=\"headerlink\" title=\"2、使用块的网络： VGG (和AlexNet 没有很大的差别)\"></a>2、使用块的网络： VGG (和AlexNet 没有很大的差别)</h3><p><img src=\"/ML-Dl-%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/vgg.svg\" alt=\"vgg\"></p>\n<h4 id=\"小结-2\"><a href=\"#小结-2\" class=\"headerlink\" title=\"小结\"></a>小结</h4><ul>\n<li>VGG-11使用可复用的卷积块构造网络。不同的VGG模型可通过每个块中卷积层数量和输出通道数量的差异来定义。</li>\n<li>块的使用导致网络定义的非常简洁。使用块可以有效地设计复杂的网络。</li>\n<li>在VGG论文中，Simonyan和Ziserman尝试了各种架构。特别是他们发现深层且窄的卷积（即3×3）比较浅层且宽的卷积更有效。</li>\n</ul>\n<h4 id=\"3、网络中的网络-NiN\"><a href=\"#3、网络中的网络-NiN\" class=\"headerlink\" title=\"3、网络中的网络 - NiN\"></a>3、网络中的网络 - NiN</h4>",
            "tags": [
                "CNN",
                "卷积神经网络"
            ]
        }
    ]
}