{
    "version": "https://jsonfeed.org/version/1",
    "title": "Supernova • All posts by \"语音处理\" tag",
    "description": "小派过气画家",
    "home_page_url": "https://www.kitx86.com",
    "items": [
        {
            "id": "https://www.kitx86.com/2022/05/11/ml-dl-audio-de-na-xie-shi/",
            "url": "https://www.kitx86.com/2022/05/11/ml-dl-audio-de-na-xie-shi/",
            "title": "ML-DL-audio的那些事",
            "date_published": "2022-05-11T00:55:26.000Z",
            "content_html": "<h4 id=\"内篇-amp-amp-齐物论\"><a href=\"#内篇-amp-amp-齐物论\" class=\"headerlink\" title=\"内篇&amp;&amp;齐物论\"></a>内篇&amp;&amp;齐物论</h4><p>源址：<a href=\"https://gitee.com/mindspore/models/tree/master/research/audio\">https://gitee.com/mindspore/models/tree/master/research/audio</a></p>\n<p>下面的几个模型是列在 gitee / audio上面的，我拿来看了前面几个</p>\n<h3 id=\"1-FastSpeech\"><a href=\"#1-FastSpeech\" class=\"headerlink\" title=\"1.FastSpeech\"></a>1.FastSpeech</h3><h4 id=\"描述\"><a href=\"#描述\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>基于神经网络的端到端文本到语音（TTS）显着提高了合成语音的质量。 TTS 方法通常首先从文本生成梅尔谱图，然后使用声码器（如 WaveNet（该工作中的 WaveGlow））从梅尔谱图合成语音。与传统的连接和统计参数方法相比，基于神经网络的端到端模型推理速度慢，合成语音通常鲁棒性不强（即某些单词被跳过或重复）和缺乏可控性（语音速度或韵律控制）。在这项工作中，我们使用基于 Transformer 的前馈网络为 TTS 并行生成梅尔谱图。具体来说，我们使用先前从基于编码器-解码器的教师模型中提取的注意力对齐来进行音素持续时间预测，长度调节器使用该模型来扩展源音素序列以匹配并行梅尔谱图的目标梅尔谱图序列的长度一代。在 LJSpeech 数据集上的实验表明，并行模型在语音质量方面与自回归模型相匹配，在特别困难的情况下几乎消除了跳字和重复的问题，并且可以平滑地调整语音速度。</p>\n<h4 id=\"模型架构\"><a href=\"#模型架构\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>FastSpeech 的架构是基于 Transformer 中的自注意力和一维卷积的前馈结构。这种结构称为前馈变压器（FFT）。 Feed-Forward Transformer 堆叠多个 FFT 块用于音素到 mel 谱图的转换，音素侧有 N 个块，mel 谱图侧有 N 个块，中间有一个长度调节器来弥合音素和 mel 之间的长度差距-频谱图序列。每个 FFT 块由一个自注意力和一维卷积网络组成。自注意力网络由一个多头注意力组成，用于提取交叉位置信息。与 Transformer 中的 2 层密集网络不同，FastSpeech 使用带有 ReLU 激活的 2 层 1D 卷积网络。动机是相邻的隐藏状态在语音任务中的字符/音素和梅尔谱图序列中更密切相关。</p>\n<h3 id=\"2-ctcmodel\"><a href=\"#2-ctcmodel\" class=\"headerlink\" title=\"2.ctcmodel\"></a>2.ctcmodel</h3><h4 id=\"描述-1\"><a href=\"#描述-1\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>CTCModel利用CTC准则训练RNN模型，完成语素标记任务。CTC 的全称是Connectionist Temporal Classification，中文名称是“连接时序分类”，这个方法主要是解决神经网络label 和output 不对齐的问题，其优点是不用强制对齐标签且标签可变长，仅需输入序列和监督标签序列即可进行训练。 CTC被广泛的应用在语音识别，OCR等任务上，取得了显著的效果</p>\n<h4 id=\"模型架构-1\"><a href=\"#模型架构-1\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>一个两层的双向LSTM模型，输入维度为39，即提取出的语音特征的维度<br>一个全连接层，输出维度为62,标签数+1,61代表空白符号</p>\n<h3 id=\"3-deepspeech2\"><a href=\"#3-deepspeech2\" class=\"headerlink\" title=\"3.deepspeech2\"></a>3.deepspeech2</h3><h4 id=\"描述-2\"><a href=\"#描述-2\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>DeepSpeech2是一个使用 CTC 损失训练的语音识别模型。它用神经网络取代了整个手工设计的管道，可以处理各种各样的语音，包括嘈杂的环境、口音和不同的语言。</p>\n<h4 id=\"模型架构-2\"><a href=\"#模型架构-2\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><ul>\n<li>两个卷积层:<ul>\n<li>通道数为 32，内核大小为 41, 11 ，步长为 2, 2</li>\n<li>通道数为 32，内核大小为 41, 11 ，步长为 2, 1</li>\n</ul>\n</li>\n<li>五个双向 LSTM 层（大小为 1024）</li>\n<li>一个投影层【大小为字符数加 1（为CTC空白符号)，29】</li>\n</ul>\n<h3 id=\"4-dscnn\"><a href=\"#4-dscnn\" class=\"headerlink\" title=\"4.dscnn\"></a>4.dscnn</h3><h4 id=\"描述-3\"><a href=\"#描述-3\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>DS-CNN 是深度可分离的卷积神经网络，于 2017 年首次用于关键字识别。KWS 应用具有高度受限的功率预算，通常在内存和计算能力有限的微型微控制器上运行。深度可分离卷积在参数数量和操作方面都更有效，这使得即使在资源受限的微控制器设备中也可以实现更深、更宽的架构。</p>\n<h3 id=\"5-fcn-4\"><a href=\"#5-fcn-4\" class=\"headerlink\" title=\"5.fcn-4\"></a>5.fcn-4</h3><h4 id=\"描述-4\"><a href=\"#描述-4\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>这个存储库提供了一个脚本和配方来训练 FCN-4 模型以实现最先进的准确性。<br>论文：“Keunwoo Choi、George Fazekas 和 Mark Sandler，“使用深度卷积神经网络进行自动标记”，国际音乐协会信息检索会议。ISMIR，2016 年。</p>\n<h4 id=\"模型架构-3\"><a href=\"#模型架构-3\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>FCN-4 是一种卷积神经网络架构，它的名称 FCN-4 来自于它有 4 层的事实。它的层由卷积层、最大池化层、激活层、全连接层组成。</p>\n<h3 id=\"6-speech-transformer\"><a href=\"#6-speech-transformer\" class=\"headerlink\" title=\"6.speech_transformer\"></a>6.speech_transformer</h3><h4 id=\"描述-5\"><a href=\"#描述-5\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>标准的transformer sequence2sequence（编码器，解码器）模型架构用于解决speech2text问题。</p>\n<h4 id=\"模型架构-4\"><a href=\"#模型架构-4\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>具体来说，Transformer 包含六个编码器模块和六个解码器模块。每个编码器模块由一个自注意力层和一个前馈层组成，每个解码器模块由一个自注意力层、一个编码器-解码器-注意力层和一个前馈层组成。</p>\n<h3 id=\"7-tacotron2\"><a href=\"#7-tacotron2\" class=\"headerlink\" title=\"7.tacotron2\"></a>7.tacotron2</h3><h4 id=\"描述-6\"><a href=\"#描述-6\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>Tacotron2 是一款 TTS 机型。它包含两个阶段，在第一阶段它使用序列到序列方法从文本序列中预测梅尔频谱图，在第二阶段它应用 WaveNet 作为声码器将梅尔频谱图转换为波形。我们支持在 Ascend 平台上训练和评估 tacotron2 模型。<br>论文：乔纳森等人。通过在梅尔谱图预测上调节 WaveNet 进行自然 TTS 合成。</p>\n<h4 id=\"模型架构-5\"><a href=\"#模型架构-5\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>Tacotron2本质上是一个包含编码器和解码器的序列到序列模型，编码器由三个conv层和一个BiLSTM层实现，解码器使用两个LSTM层来解码下一个状态，在编码器之间应用位置感知注意力和解码器，然后将解码后的状态输入到由五个卷积层实现的 postnet 来预测梅尔谱图，最后将预测的梅尔谱图特征输入到 WaveNet 声码器以合成语音信号。</p>\n<h3 id=\"8-wavenet\"><a href=\"#8-wavenet\" class=\"headerlink\" title=\"8.wavenet\"></a>8.wavenet</h3><h4 id=\"描述-7\"><a href=\"#描述-7\" class=\"headerlink\" title=\"描述\"></a>描述</h4><p>WaveNet 是一种用于生成原始音频波形的深度神经网络。该模型是完全概率和自回归的，每个音频样本的预测分布都以所有先前的样本为条件。我们支持在 Ascend、GPU 和 CPU 上进行训练和评估。</p>\n<h4 id=\"模型架构-6\"><a href=\"#模型架构-6\" class=\"headerlink\" title=\"模型架构\"></a>模型架构</h4><p>当前模型包括一个预卷积层，然后是几个残差块，这些残差块具有与门控激活单元的残差和跳跃连接。最后，添加后卷积层来预测分布。</p>\n",
            "tags": [
                "语音处理",
                "model"
            ]
        }
    ]
}